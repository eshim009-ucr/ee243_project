\documentclass[english]{beamer}

\include{header}
\hypersetup{
	pdftitle = {%
		Efficient CNN Accelerators on FPGAs
	},
	pdfauthor = {Clarity Shimoniak},
	pdfsubject = {EE 243 Project Presentation},
	pdfkeywords = {%
		computer vision, convolutional neural network, CNN, FPGA, Xilinx,
		Lattice, MNIST
	},
}

\addbibresource{fpga.bib}
\addbibresource{models.bib}
\addbibresource{related.bib}


\begin{document}

\title{Efficient CNN Accelerators on FPGAs}
\subtitle{EE 243 Final Project}
\author{Clarity Shimoniak}
\date{Spring 2024}
\frame{\titlepage}


\begin{frame}
\frametitle{Problem Statement and Motivation}
\begin{itemize}
	\item FPGA accelerators are promising for embedded applications where GPUs
	are unavailable.
	\item Depthwise-separable convolutions are key to the efficiency of
	MobileNet\supercites{mobilenetv1}{mobilenetv2} \&
	EfficientNet\supercite{efficientnet}.
	\item Existing FPGA accelerators do not implement them
	efficiently\supercite{mobilenet2019fpga}.
	\item Most FPGA accelerators do not publicly release code.
	\begin{itemize}
		\item All rely on expensive, proprietary Altera or Xilinx boards.
	\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Framework Overview Figure}
\begin{figure}
	\centering
	\input{network}
\end{figure}
\begin{columns}
	\column[t]{0.5\linewidth}
	\begin{itemize}
		\item Similar to early MobileNet stage.
		\item Trained with PyTorch on Colab.
		\begin{itemize}
			\item $\SI{81}{\percent}$ accuracy on CIFAR10.
		\end{itemize}
	\end{itemize}
	\column[t]{0.5\linewidth}
	\begin{itemize}
		\item Each convolution layer is followed by a ReLu and MaxPool.
		\item Converted to 8-bit fixed point for Verilog.
	\end{itemize}
\end{columns}
\end{frame}


\begin{frame}
\frametitle{Methodology}
\begin{columns}
	\column{0.8\linewidth}
	\begin{itemize}
		\item Create a minimal model with a similar structure to MobileNet.
		\begin{itemize}
			\item Only open-source FPGA MobileNet
			accelerator\supercite{solovyev2019mobilenet} is dependent on
			proprietary \$700 board.
			\item Remove as layers and channels as possible while maintaining high
			accuracy on a popular dataset.
		\end{itemize}
		\item Use the DPU architecture proposed by
		\citeauthor{mobilenet2019fpga}\supercite{mobilenet2019fpga}
		to accelerate it.
		\begin{itemize}
			\item Compare against an un-optimized architecture for the same network.
			\item Key improvement is separate depthwise-convolution engine.
		\end{itemize}
		\item Test synthesized model against images using Icarus Verilog.
	\end{itemize}
	\column{0.2\linewidth}
	\begin{figure}
		\centering
		\input{network-pytorch}
	\end{figure}
\end{columns}
\end{frame}


\begin{frame}
\frametitle{Experiments \& Results}
\begin{itemize}
	\item Model size (in terms of weights) was not the bottleneck.
	\item Limited floorplan space and CPU time to synthesize model was primary
	bottleneck.
	\begin{itemize}
		\item A model with $\SI{6}{\kilo\byte}$ of weights took over 5 hours to
		synthesize.
		\item A model with $\SI{400}{\byte}$ of weights took 5 \emph{seconds}
		to synthesize.
	\end{itemize}
	\item Scaling down initial image has exponential floorplan benefits.
	\item Floorplan complexity could be reduced by de-parallelizing some
	computations.
\end{itemize}
\begin{columns}
\column{0.5\linewidth}
\begin{figure}
	\centering
	\input{overparallelize}
	\caption*{Overparallelized Convolutional Adder Tree: 3 Levels, 21 Adders}
\end{figure}
\column{0.5\linewidth}
\begin{figure}
	\centering
	\input{deparallelize}
	\caption*{Deparallelized Convolutional Adder Tree: 2 Levels, 5 Adders}
\end{figure}
\end{columns}
\end{frame}


\begin{frame}[allowframebreaks]
\frametitle{References}
\tiny\printbibliography
\end{frame}


\end{document}
