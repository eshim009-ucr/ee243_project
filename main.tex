\documentclass[12pt,letterpaper,english]{article}

\include{header}
\hypersetup{
	pdftitle = {%
		Decreasing Power Usage of
		Convolutional Neural Network Implementations
		on FPGAs
	},
	pdfauthor = {Clarity Shimoniak},
	pdfsubject = {EE 243 Project Proposal},
	pdfkeywords = {%
		computer vision, convolutional neural network, CNN, FPGA, Xilinx,
		Lattice, MNIST
	},
}


\addbibresource{models.bib}
\addbibresource{fpga.bib}
\addbibresource{old.bib}
\newcommand{\citework}[1]{\citeauthor{#1} \autocite{#1}}
\newcommand{\citeworks}[2]{%
	\citeauthor*{#1} \autocite{#1} and \citeauthor*{#2} \autocite{#2}%
}
\newcommand{\citemoreworks}[3]{%
	\citeauthor*{#1} \autocite{#1},
	\citeauthor*{#2} \autocite{#2}, and
	\citeauthor*{#3} \autocite{#3}%
}


\begin{document}

\begin{center}
	{\LARGE%
		Decreasing Power Usage of
		Convolutional Neural Network Implementations
		on FPGAs
	} \\
	\vspace{6pt}
	Clarity Shimoniak
	\vspace{-18pt}
\end{center}

\section*{Key Publication}

\todo[Reference to the most relevant research paper.]


\section*{Topic Introduction}

The high power usage of most CNN implementations limits their viability for
battery-powered applications. Additionally, large and expensive acceleration
hardware like GPUs limits their viability for consumer applications. Increasing
power efficiency and decreasing component cost would allow these models to be
deployed far more widely.

\citework{mobilenet2019fpga} and \citework{mobilenet2021fpga} implement the
\citework{mobilenetv1} model on FPGAs to achieve this goal.


\section*{Planned Activities}

I plan to implement a system similar to \citeauthor*{old}'s using the Lattice
iCE40UP5k and a similar OmniVision camera. If time permits, I will also
implement the system on other Lattice FPGAs of slightly higher and lower specs
(LFE5U-25F-6 and iCE40LP1k respectively). Achieving similar results with these
parts would show that the approach is generalizable to lower-cost parts from
other manufacturers using an open source toolchain.


\section*{Related Publications}

\todo[Overview of related works.]

\begin{itemize}
	\item \citeauthor{old} refers to
	SqueezeNet \autocite{squeezenet},
	MobileNet \autocites{mobilenetv1}{mobilenetv2},
	ShuffleNet \autocite{shufflenetv1}, and
	ESPNet \autocite{espnet}
	as examples of model size reduction.
\end{itemize}


\newpage
\printbibliography


\end{document}
